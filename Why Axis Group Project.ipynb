{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Fare Prediction\n",
    "COMP9417 Group Project    \n",
    "Why Axis Group    \n",
    "UNSW 2019T2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv, math, random, os\n",
    "from haversine import haversine, Unit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly pick a sample from the full dataset\n",
    "#rows = 1_000_000 # number or rows in file\n",
    "#sample_size = 500_000 # training size\n",
    "\n",
    "#skip = sorted(random.sample(range(1, rows + 1), rows - sample_size))\n",
    "\n",
    "#df_raw_train = pd.read_csv('../input/train.csv', nrows=5_000_000)\n",
    "df_raw_test = pd.read_csv('datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_raw_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fd04d6ca5144>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_raw_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_raw_train' is not defined"
     ]
    }
   ],
   "source": [
    "df_raw_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Lets remove anomalies to make sure the data we are using will not affect our model during training. These are few things that we should consider:\n",
    "* null and missing values\n",
    "* duplicate data\n",
    "* minimum taxi fare should be $2.50 (https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page)\n",
    "* passenger_count must be greater than or equal to 1 and less than or equal to 4 (0 means the ride was either cancelled or rejected, a car can only fit at most 4 passengers)\n",
    "* latitude and longitude coordinates should be within NYC (https://www.latlong.net/place/new-york-city-ny-usa-1848.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The center coordinates of NYC are (40.730610, -73.935242), we pick a margin from this center to capture all the data points within this margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC_LONG = -73.935242\n",
    "NYC_LAT = 40.730610\n",
    "\n",
    "# margin from the center coordinates. \n",
    "# increase this value if you want to capture more data\n",
    "MARGIN = 0.8\n",
    "\n",
    "# longitude range\n",
    "min_long = float(NYC_LONG - MARGIN)\n",
    "max_long = float(NYC_LONG + MARGIN)\n",
    "\n",
    "# latitude range\n",
    "min_lat = float(NYC_LAT - MARGIN)\n",
    "max_lat = float(NYC_LAT + MARGIN)\n",
    "\n",
    "def data_cleaning(df):\n",
    "    # remove null values\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # remove duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # remove longitude that is out of range\n",
    "    df = df[(df.pickup_longitude < max_long) & (df.pickup_longitude > min_long)]\n",
    "    df = df[(df.dropoff_longitude < max_long) & (df.dropoff_longitude > min_long)]\n",
    "\n",
    "    # remove latitude that is out of range\n",
    "    df = df[(df.pickup_latitude < max_lat) & (df.pickup_latitude > min_lat)]\n",
    "    df = df[(df.dropoff_latitude < max_lat) & (df.dropoff_latitude > min_lat)]\n",
    "\n",
    "    # remove amount less than the minimum taxi fare\n",
    "    if 'fare_amount' in df:\n",
    "        df.drop(df[df.fare_amount < 2.50].index, inplace=True)\n",
    "        df.drop(df[df.fare_amount > 300.00].index, inplace=True)\n",
    "\n",
    "    # remove passenger count less than 1 and greater than 4\n",
    "    df.drop(df[df.passenger_count < 1].index, inplace=True) # has to be one passenger in the car\n",
    "    df.drop(df[df.passenger_count > 4].index, inplace=True) # not more than 4 passengers can fit in a car\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "The features in our data are important to the predictive models we use and will influence the results we are going to achieve. The quality and quantity of the features will have great influence on whether the model is good or not.\n",
    "\n",
    "We could say the better the features are, the better the result is. This isn't entirely true, because the results achieved also depend on the model and the data, not just the chosen features. That said, choosing the right features is still very important. Better features can produce simpler and more flexible models, and they often yield better results. (https://en.wikipedia.org/wiki/Feature_engineering)\n",
    "\n",
    "Date and time of pickups is also a key feature because some charges are different for weekends and weekdays and some depend on the time of day whether you are catching a taxi during the day or at night time.\n",
    "\n",
    "Trips to the three main airports in New York have different rates from the standard rate https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page. We need to ensure these fixed rates will not affect the standard rates so we include pickups and dropoffs from these main airports as separate features. The three main airports are:\n",
    "* LaGuardia Airport (LGA) (40.77064 -73.86764)\n",
    "* John F. Kennedy Airport (JFK) (40.64459 -73.78295)\n",
    "* Newark Airport (EWR) (40.69211 -74.18288)   \n",
    "\n",
    "Coordinates from (https://latitude.to/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocation Distance\n",
    "Distance between pickups and dropoffs is the major factor in determining taxi fares. Before adding these features we need to define our distance function to calculate the distance between each pickups and dropoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(df):\n",
    "    pickup_location = (df['pickup_latitude'], df['pickup_longitude'])\n",
    "    dropoff_location = (df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    distance = haversine(pickup_location, dropoff_location)\n",
    "    return distance\n",
    "\n",
    "def haversine(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n",
    "    pickup_lat, pickup_long, dropoff_lat, dropoff_long = map(np.radians, [pickup_lat, pickup_long, dropoff_lat, dropoff_long])\n",
    "    lat_dist = dropoff_lat - pickup_lat\n",
    "    long_dist = dropoff_long - pickup_long\n",
    "    a = np.sin(lat_dist/2.0)**2 + np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(long_dist/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    distance = r * c\n",
    "    return distance\n",
    "\n",
    "def manhattan(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n",
    "    return np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n",
    "\n",
    "def euclidean(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n",
    "    return pow(pow((dropoff_lat - pickup_lat), 2) + pow((dropoff_long - pickup_long), 2), 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGA_lat = 40.77064 \n",
    "LGA_long = -73.86764\n",
    "JFK_lat = 40.64459\n",
    "JFK_long = -73.78295\n",
    "EWR_lat = 40.69211\n",
    "EWR_long = -74.18288\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # split pickup_datetime    \n",
    "    df['pickup_datetime'] =  pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S %Z')\n",
    "    df['year'] = df['pickup_datetime'].apply(lambda x: x.year)\n",
    "    df['month'] = df['pickup_datetime'].apply(lambda x: x.month)\n",
    "    df['day'] = df['pickup_datetime'].apply(lambda x: x.day)\n",
    "    df['hour'] = df['pickup_datetime'].apply(lambda x: x.hour)\n",
    "    df['day_of_week'] = df['pickup_datetime'].apply(lambda x: x.dayofweek)\n",
    "    \n",
    "    # add distances to major airports\n",
    "    df['distance'] = haversine(df.pickup_latitude, df.pickup_longitude, df.dropoff_latitude, df.dropoff_longitude) \n",
    "    df['manhattan_dist'] = manhattan(df.pickup_latitude, df.pickup_longitude, df.dropoff_latitude, df.dropoff_longitude) \n",
    "    #df['euclidean_dist'] = euclidean(df.pickup_latitude, df.pickup_longitude, df.dropoff_latitude, df.dropoff_longitude) \n",
    "    df['pickup_LGA_dist'] = haversine(df['pickup_latitude'], df['pickup_longitude'], LGA_lat, LGA_long)\n",
    "    df['dropoff_LGA_dist'] = haversine(df['dropoff_latitude'], df['dropoff_longitude'], LGA_lat, LGA_long)\n",
    "    df['pickup_JFK_diste'] = haversine(df['pickup_latitude'], df['pickup_longitude'], JFK_lat, JFK_long)\n",
    "    df['dropoff_JFK_dist'] = haversine(df['dropoff_latitude'], df['dropoff_longitude'], JFK_lat, JFK_long)\n",
    "    df['pickup_EWR_dist'] = haversine(df['pickup_latitude'], df['pickup_longitude'], EWR_lat, EWR_long)\n",
    "    df['dropoff_EWR_dist'] = haversine(df['dropoff_latitude'], df['dropoff_longitude'], EWR_lat, EWR_long)\n",
    "    \n",
    "    # 50 cents overnight surcharge 8pm to 6am. 1 indicating theres a surcharge and 0 otherwise.\n",
    "    df['overnight'] = np.where((df['hour'] >= 20) | (df['hour'] <= 6), 1, 0)\n",
    "    \n",
    "    # trips that goes in circle in the same area\n",
    "    # indicated by high fare but very small distance, benchmark is if you go from Manhattan \n",
    "    # to JFK airport with approximately $60 fare for approximately 30kilometers\n",
    "    # as indicated by TLC\n",
    "    if 'fare_amount' in df:\n",
    "        df.drop(df[(df.distance < 10) & (df.fare_amount > 150)].index, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_temp = data_cleaning(df_raw_train)\n",
    "df_temp = feature_engineering(df_clean_temp)\n",
    "df_temp.plot.scatter('distance', 'fare_amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the distribution above our hypothesis seem to be correct. There is a linear distribution between distance and the fare amount. Lets apply the preprocessing and feature engineering to both our training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the original dataframe, if any changes are made\n",
    "#df_train = df_raw_train.copy(deep=True)\n",
    "df_test = df_raw_test.copy(deep=True)\n",
    "\n",
    "#df_clean_train = data_cleaning(df_train)\n",
    "#df_train = feature_engineering(df_clean_train)\n",
    "df_clean_test = data_cleaning(df_test)\n",
    "df_test = feature_engineering(df_clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check corr of 'fare_amount' to all the other variables\n",
    "print(df_train.corrwith(df_train['fare_amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to scale the features to ensure they have an uniform range so that large number features do not dominate features with low numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-673ee873edc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'key'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'fare_amount'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'pickup_datetime'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'passenger_count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfare_amount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# scale features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "cols = ['key','fare_amount','pickup_datetime', 'passenger_count']\n",
    "df_feature = df_train.drop(columns=cols)\n",
    "df_target = df_train.fare_amount\n",
    "\n",
    "# scale features\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "cols = df_feature.columns.tolist()\n",
    "df_feature[cols] = scaler.fit_transform(df_feature[cols])\n",
    "# df_scaled = pd.concat([df_feature, df_target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split our data into train and test subsets. (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with 20% test and 80% train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_feature, df_target, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation= 'relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(128, activation= 'relu'))\n",
    "model.add(Dense(64, activation= 'relu'))\n",
    "model.add(Dense(32, activation= 'relu'))\n",
    "#model.add(Dense(16, activation= 'relu'))\n",
    "#model.add(Dense(8, activation= 'relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#LEARNING_RATE = 0.01\n",
    "#adam = Adam(lr=LEARNING_RATE)\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy', 'mae'])\n",
    "history = model.fit(X_train, y_train, epochs=60, batch_size=256, validation_split=0.3)\n",
    "# random sample data for bagging\n",
    "#df.sample(frac=0.5, replace=True, random_state=1)\n",
    "\n",
    "# check\n",
    "# https://github.com/dimitreOliveira/NewYorkCityTaxiFare/blob/master/keras_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('60e_5mil_loss12.0425_valloss_12.5486.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "train_pred = model.predict(X_train)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "test_pred = model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "print(\"Train RMSE: {:0.2f}\".format(train_rmse))\n",
    "print(\"Test RMSE: {:0.2f}\".format(test_rmse))\n",
    "print('------------------------')\n",
    "def predict(df, X_test, model):\n",
    "    sample = X_test.sample(n=1, random_state=np.random.randint(low=0, high=10000))\n",
    "    idx = sample.index[0]\n",
    "\n",
    "    actual_fare = df.loc[idx,'fare_amount']\n",
    "    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    day_of_week = day_names[df.loc[idx,'day_of_week']]\n",
    "    hour = df.loc[idx,'hour']\n",
    "    predicted_fare = model.predict(sample)[0][0]\n",
    "    rmse = np.sqrt(np.square(predicted_fare - actual_fare))\n",
    "\n",
    "    print(\"Trip Details: {}, {}:00hrs\".format(day_of_week, hour))  \n",
    "    print(\"Actual fare: ${:0.2f}\".format(actual_fare))\n",
    "    print(\"Predicted fare: ${:0.2f}\".format(predicted_fare))\n",
    "    print(\"RMSE: ${:0.2f}\".format(rmse))\n",
    "    \n",
    "\n",
    "# randomly predict from test sample\n",
    "predict(df_train, X_test, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = taxi['fare_amount']\n",
    "#X = taxi.drop(columns=['fare_amount'])\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df_feature, df_target, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"Test RMSE: %.3f\" % mean_squared_error(y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the \"Training data\"\n",
    "X = X_train.distance\n",
    "# target data is array of shape (n,) \n",
    "y = pd.DataFrame(y_train, columns=['fare_amount'])\n",
    "\n",
    "#plt.scatter(X, y,color='g')\n",
    "#plt.plot(X, linear_model.predict(X),color='k')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(max_depth=10, random_state=0, n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Test RMSE: %.3f\" % mean_squared_error(y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b2dcc5d67f0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots of history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Ensemble\n",
    "https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/\n",
    "https://www.youtube.com/watch?v=2Mg8QD0F1dQ\n",
    "Split n training data into m number of bags containing randomly picked n' data.    \n",
    "n - number of instances    \n",
    "n' - number in a bag    \n",
    "m - number of bags    \n",
    "n'< n usually by 60%    \n",
    "\n",
    "We train on these bags to have m number of models. Do prediction on these models and the final predition will be the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple train-test splits\n",
    "n_splits = 10\n",
    "m_bags = 4500\n",
    "scores, members = list(), list()\n",
    "for _ in range(n_splits):\n",
    "    # select indexes\n",
    "    ix = [i for i in range(len(X_train))]\n",
    "    train_ix = resample(ix, replace=True, n_samples=m_bags)\n",
    "    test_ix = [x for x in ix if x not in train_ix]\n",
    "    # select data\n",
    "    trainX, trainy = X[train_ix], y[train_ix]\n",
    "    testX, testy = X[test_ix], y[test_ix]\n",
    "    # evaluate model\n",
    "    model, test_acc = evaluate_model(trainX, trainy, testX, testy)\n",
    "    print('>%.3f' % test_acc)\n",
    "    scores.append(test_acc)\n",
    "    members.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('loss')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test submission on kaggle (previous score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 02:53:36.830245 17100 deprecation_wrapper.py:119] From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0802 02:53:36.876250 17100 deprecation_wrapper.py:119] From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0802 02:53:37.077245 17100 deprecation_wrapper.py:119] From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0802 02:53:37.080249 17100 deprecation_wrapper.py:119] From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0802 02:53:37.082247 17100 deprecation_wrapper.py:119] From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0802 02:53:37.307245 17100 deprecation_wrapper.py:119] From c:\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.629625]\n",
      " [12.599325]\n",
      " [ 9.005811]\n",
      " ...\n",
      " [26.315952]\n",
      " [12.107865]\n",
      " [13.970414]]\n"
     ]
    }
   ],
   "source": [
    "cols = ['key','pickup_datetime', 'passenger_count']\n",
    "X_test_sub = df_test.copy(deep=True)\n",
    "key = X_test_sub.key\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_test_sub.drop(columns=cols, inplace=True)\n",
    "X_test_sub_cols = X_test_sub.columns.tolist()\n",
    "\n",
    "X_test_sub[X_test_sub_cols] = scaler.fit_transform(X_test_sub[X_test_sub_cols])\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('60e_5mil_loss12.0425_valloss_12.5486.h5')\n",
    "\n",
    "y_test_sub = model.predict(X_test_sub)\n",
    "#sub = pd.DataFrame(\n",
    "#    {'key': key, 'fare_amount': y_test_sub[:, 0]},\n",
    "#    columns = ['key', 'fare_amount'])\n",
    "#sub.to_csv('1-08-19_submission.csv', index=False)\n",
    "print(y_test_sub)\n",
    "#print(os.listdir('.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.gitignore', '.ipynb_checkpoints', '1-08-19_submission.csv', '100epochs_model.h5', '1mil_model.h5', '60e_5mil_loss12.0425_valloss_12.5486.h5', 'analyse.py', 'chapter', 'datasets', 'images', 'main.py', 'neural_network.py', 'preprocess.py', 'README.md', 'requirements.txt', 'submission_6.84531.csv', 'visual.py', 'Why Axis Group Project.ipynb', 'whyaxis_submission.csv', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
